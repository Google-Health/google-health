{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHQ6pgw0SFLD"
      },
      "source": [
        "This notebook assumes that you have downloaded CIDRZ data available at https://www.kaggle.com/datasets/googlehealthai/google-health-ai?resource=download\n",
        "\n",
        "More specifically, for this example, we will use audio recordings from the `Chainda South Phone B` and `Kanyama Phone B` directories, and metadata from `Metadata and Codebook`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4pojuQESBTJ"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import concurrent.futures\n",
        "import io\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# copybara:strip_begin(Internal imports)\n",
        "from colabtools import drive\n",
        "# copybara:strip_end\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io.wavfile as wavfile\n",
        "from scipy.signal import resample\n",
        "from sklearn import metrics\n",
        "import tensorflow as tf\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkLSUl2zSpgR"
      },
      "outputs": [],
      "source": [
        "# copybara:strip_begin(Internal imports)\n",
        "metadata_zip_bytes = drive.LoadFile(file_id='1bE5hlnBpUsgfC-5GOtjTuR2z4ZPcCkUe')\n",
        "# copybara:strip_end_and_replace_begin\n",
        "# # This cell assumes that you have downloaded the metadata zip file at `/path/metadata.zip`\n",
        "# # Please update the path accordingly.\n",
        "# with open('/path/metadata.zip', 'rb') as zfile:\n",
        "#   metadata_zip_bytes = zfile.read()\n",
        "# copybara:replace_end\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYpjEaKbS6JJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def read_csvs_from_zip(zip_bytes):\n",
        "    \"\"\"Reads CSV files from a zip archive into pandas DataFrames.\n",
        "\n",
        "    Args:\n",
        "        zip_bytes: The bytes representing the zip file.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are filenames and values are pandas DataFrames.\n",
        "        Returns an empty dictionary if the zip file is invalid or contains no CSVs.\n",
        "        Prints error messages if CSV reading fails for a specific file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        zf = zipfile.ZipFile(io.BytesIO(zip_bytes))\n",
        "        csv_data = {}\n",
        "\n",
        "        for info in zf.infolist():\n",
        "\n",
        "            if info.filename.endswith(\".csv\"):\n",
        "                try:\n",
        "                    with zf.open(info) as csvfile:  # No need for extractall\n",
        "                        df = pd.read_csv(csvfile)\n",
        "                        csv_data[info.filename] = df\n",
        "                except pd.errors.ParserError as e: # Catch CSV parsing errors\n",
        "                    print(f\"Error reading CSV {info.filename}: {e}\")\n",
        "                except Exception as e: # Catch other errors\n",
        "                    print(f\"Error processing file {info.filename}: {e}\")\n",
        "\n",
        "        return csv_data\n",
        "\n",
        "    except zipfile.BadZipFile:\n",
        "        print(\"Error: Invalid zip file\")\n",
        "        return {}\n",
        "\n",
        "\n",
        "dfs = read_csvs_from_zip(metadata_zip_bytes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKoSQ-SrTThZ"
      },
      "outputs": [],
      "source": [
        "dfs.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlItZN8MTacT"
      },
      "outputs": [],
      "source": [
        "dfs['Metadata and Codebook/Google_Health_AI_Final_Codebook.csv']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvDFAdWBTb_d"
      },
      "outputs": [],
      "source": [
        "dfs['Metadata and Codebook/GHAI_Final_Data_2023.csv']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAfuW9mdTdAl"
      },
      "outputs": [],
      "source": [
        "dfs['Metadata and Codebook/GHAI_Final_Data_2023.csv']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biUyymqnTpcz"
      },
      "source": [
        "# Read audio\n",
        "\n",
        "* convert to mono\n",
        "* resamples to 16 kHz\n",
        "* represents as numpy arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jk8cnUCfTrUS"
      },
      "outputs": [],
      "source": [
        "# copybara:strip_begin(Internal imports)\n",
        "chainda_B_zip_bytes = drive.LoadFile(\n",
        "    file_id='1PpQz7KuZKzX47lImrj5mZzQRYS3t7N8s'\n",
        ")\n",
        "kanyama_B_zip_bytes = drive.LoadFile(file_id='1DCJk8wnZQdSBaj-uiEr4HRgVkGntHb3Q')\n",
        "# copybara:strip_end_and_replace_begin\n",
        "# # This cell assumes that you have downloaded the metadata zip file at `/path/chainda_B.zip`\n",
        "# # and `/path/kanyama_B.zip`\n",
        "# # Please update the paths accordingly.\n",
        "# with open('/path/chainda_B.zip', 'rb') as zfile:\n",
        "#   chainda_B_zip_bytes = zfile.read()\n",
        "# with open('/path/kanyama_B.zip', 'rb') as zfile:\n",
        "#   kanyama_B_zip_bytes = zfile.read()\n",
        "# copybara:replace_end\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCgvySoeT9s-"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def process_zipped_wavs(zip_bytes: bytes) -\u003e dict[str, np.ndarray]:\n",
        "    \"\"\"Processes a zip file containing WAV files and downsamples them to 16kHz.\n",
        "\n",
        "    Args:\n",
        "        zip_bytes: Bytes representing the zip file.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are filenames and values are NumPy arrays\n",
        "        containing the processed audio data (16kHz, last 15s).\n",
        "        Returns an empty dictionary if there are errors or no WAV files are found.\n",
        "    \"\"\"\n",
        "    try:\n",
        "      zf = zipfile.ZipFile(io.BytesIO(zip_bytes))\n",
        "      wav_data = {}\n",
        "      for info in zf.infolist():\n",
        "        if info.filename.endswith(\".wav\"):\n",
        "\n",
        "          with zf.open(info) as wav_file:\n",
        "            try:\n",
        "              rate, data = wavfile.read(wav_file)\n",
        "\n",
        "              if data.dtype != np.float32:\n",
        "                if data.dtype == np.int16:\n",
        "                  data = data.astype(np.float32) / 32768.0\n",
        "                elif data.dtype == np.int32:\n",
        "                  data = data.astype(np.float32) / 2147483648.0\n",
        "\n",
        "              # Handle multi-channel WAV files (e.g., stereo)\n",
        "              if data.ndim \u003e 1:\n",
        "                data = np.mean(data, axis=1)\n",
        "\n",
        "              # Downsample to 16kHz\n",
        "              if rate != 16000:\n",
        "                num_samples_new = int(len(data) * 16000 / rate)\n",
        "                data = resample(data, num_samples_new)\n",
        "\n",
        "              wav_data[info.filename.replace(' ', '_')] = data\n",
        "\n",
        "            except Exception as e:  # Handle potential WAV read errors\n",
        "              print(f\"Error reading WAV file {info.filename}: {e}\")\n",
        "\n",
        "      return wav_data\n",
        "\n",
        "    except zipfile.BadZipFile:  # Handle invalid zip files\n",
        "      print(\"Error: Invalid zip file\")\n",
        "      return {}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiSW8sKfUbXO"
      },
      "outputs": [],
      "source": [
        "processed_wavs = process_zipped_wavs(kanyama_B_zip_bytes)\n",
        "\n",
        "for filename, audio_data in processed_wavs.items():\n",
        "  print(f\"File: {filename}, Shape: {audio_data.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aletgXJUaak"
      },
      "outputs": [],
      "source": [
        "processed_wavs_test = process_zipped_wavs(chainda_B_zip_bytes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tvk7-5hIUdYO"
      },
      "outputs": [],
      "source": [
        "for k, v in processed_wavs.items():\n",
        "  plt.plot(v)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTCxAeOUUepO"
      },
      "source": [
        "# Process audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxT6pMX_Ugw-"
      },
      "source": [
        "## Extract final sequence of coughs\n",
        "\n",
        "As part of the CIDRZ protocol, participants are required to cough once, later, another one, and finally, they are asked to repeatedly cough.\n",
        "\n",
        "In https://arxiv.org/abs/2403.02522, we found that the final sequence of coughs resulted in better performance, hypothetically because this \"forced\" sequence of cough also elicit involuntary coughs, which have been shown to be more predictive of disease status in https://www.science.org/doi/10.1126/sciadv.adi0282.\n",
        "\n",
        "In our experiments, we had access to a cough detector, which outputs a score between 0 and 1 indicating how likely a 2s 16kHz audio clip is to contain a cough event. Since we do not have access to this model here, we will use a simple heuristic to extract the final sequence of coughs from the audio files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-tWPkJhVIrn"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compute_spectrogram(\n",
        "    audio: np.ndarray | tf.Tensor,\n",
        "    frame_length: int = 400,\n",
        "    frame_step: int = 160,\n",
        "    ):\n",
        "\n",
        "  if len(audio.shape) == 2:\n",
        "    audio = np.mean(audio, axis=1)\n",
        "  elif len(audio.shape) \u003e 2:\n",
        "    raise NotImplementedError(\n",
        "        f'`audio` should have at most 2 dimensions but had {len(audio.shape)}')\n",
        "  stft_output = tf.signal.stft(\n",
        "      audio,\n",
        "      frame_length=frame_length,\n",
        "      frame_step=frame_step,\n",
        "      fft_length=frame_length)\n",
        "  spectrogram = tf.abs(stft_output)\n",
        "  return spectrogram\n",
        "\n",
        "\n",
        "def compute_loudness(\n",
        "    audio: np.ndarray | tf.Tensor,\n",
        "    sample_rate: float = 16000.0,\n",
        ") -\u003e np.ndarray:\n",
        "  \"\"\"Computes loudness.\n",
        "\n",
        "  It is defined as the per-channel per-timestep cross-frequency L2 norm of the\n",
        "  log mel spectrogram.\n",
        "\n",
        "  Args:\n",
        "    audio: Array of shape [num_timesteps] representing a raw wav\n",
        "      file.\n",
        "    sample_rate: The sample rate of the input audio.\n",
        "    fft_output_conversion: The string indicating the output conversion function.\n",
        "      Currently, only `magnitude` and `magnitude_squared` are supported.\n",
        "\n",
        "  Returns:\n",
        "    An array of shape [num_timesteps] representing the loudness.\n",
        "  \"\"\"\n",
        "  frame_step = int(sample_rate) // 100  # 10 ms\n",
        "  frame_length = 25 * int(sample_rate) // 1000  # 25 ms\n",
        "  linear_spectrogram = compute_spectrogram(\n",
        "      audio.astype(np.float32),\n",
        "      frame_length=frame_length,\n",
        "      frame_step=frame_step,\n",
        "  )\n",
        "  print(audio.shape, audio.shape[0] //16000, linear_spectrogram.shape)\n",
        "  sum_amplitude = np.sum(linear_spectrogram, axis=1)\n",
        "  loudness_db_timeseries = 20 * np.log10(sum_amplitude)\n",
        "  return np.asarray(loudness_db_timeseries)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aE5BjyBkWFFT"
      },
      "outputs": [],
      "source": [
        "loudness = {}\n",
        "for k, v in processed_wavs.items():\n",
        "  loudness[k] = compute_loudness(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_R7fDV7WFi5"
      },
      "outputs": [],
      "source": [
        "loudness_test = {}\n",
        "for k, v in processed_wavs_test.items():\n",
        "  loudness_test[k] = compute_loudness(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqkKv_1TWG2y"
      },
      "outputs": [],
      "source": [
        "LOUDNESS_THRESHOLD = 42\n",
        "\n",
        "for i, (participant_id, loudness_series) in enumerate(loudness.items()):\n",
        "  plt.plot(loudness_series)\n",
        "  plt.axhline(LOUDNESS_THRESHOLD, c='k', linestyle='--')\n",
        "  plt.show()\n",
        "  if i \u003e 20:\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJIaFL6lWNaU"
      },
      "source": [
        "We can see that the peaks above `LOUDNESS_THRESHOLD` most likely correspond to coughs, and we want to extract the final ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DQ0rSmVWPsw"
      },
      "outputs": [],
      "source": [
        "\n",
        "def extract_final_loud_clips_information(\n",
        "    loudness: np.ndarray,\n",
        "    min_peak_height: float = LOUDNESS_THRESHOLD,\n",
        "    window_size: int = 200,\n",
        "    window_step: int = 100,\n",
        "    number_of_peaks: int = 5,\n",
        ") -\u003e list[dict[str, np.ndarray | int]]:\n",
        "  \"\"\"Extracts final sequence of coughs from the loudness timeseries.\n",
        "\n",
        "  Args:\n",
        "    loudness: Array of shape [num_timesteps] representing the loudness.\n",
        "    min_peak_height: Minimal amplitude of a peak to be considered a likely cough\n",
        "    window_size: Size of the window. 100 corresponds to 1s.\n",
        "    window_step: Step of the window. 100 corresponds to 1s.\n",
        "    number_of_peaks: Number of peaks to extract.\n",
        "  \"\"\"\n",
        "  picked_windows = []\n",
        "  for i in range(loudness.size//window_step):\n",
        "    end = loudness.size - i * window_step\n",
        "    start = end - window_size\n",
        "    window = loudness[start: end]\n",
        "    if np.max(window) \u003e min_peak_height:\n",
        "      picked_windows.append({\n",
        "          'window': window,\n",
        "          # Multiply by 160 to convert back to the initial temporal scale\n",
        "          'start': 160 * start,\n",
        "          'end': 160 * end,\n",
        "      })\n",
        "    if len(picked_windows) \u003e= number_of_peaks:\n",
        "      return picked_windows\n",
        "  return picked_windows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5N5QuQi1Wtaz"
      },
      "outputs": [],
      "source": [
        "final_loud_clips_data = {}\n",
        "audio_clips_per_participant = {}\n",
        "\n",
        "for i, (pid, series) in enumerate(loudness.items()):\n",
        "  try:\n",
        "    final_loud_clips_data[pid] = extract_final_loud_clips_information(series)\n",
        "  except:\n",
        "    continue\n",
        "  audio_clips = []\n",
        "  for clip in final_loud_clips_data[pid]:\n",
        "    start = clip['start']\n",
        "    end = clip['end']\n",
        "    wav = processed_wavs[pid]\n",
        "    audio_clips.append(wav[start:end])\n",
        "    if i \u003c 3:\n",
        "      fig, axes = plt.subplots(nrows=1, ncols=2)\n",
        "      axes[0].plot(clip['window'])\n",
        "      print(pid)\n",
        "      print(start / wav.size, end / wav.size)\n",
        "      axes[1].plot(wav[start: end])\n",
        "      plt.show()\n",
        "  audio_clips_per_participant[pid] = audio_clips\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjW6O7bbWuBR"
      },
      "outputs": [],
      "source": [
        "final_loud_clips_data_test = {}\n",
        "audio_clips_per_participant_test = {}\n",
        "\n",
        "for i, (pid, series) in enumerate(loudness_test.items()):\n",
        "  try:\n",
        "    final_loud_clips_data_test[pid] = extract_final_loud_clips_information(series)\n",
        "  except:\n",
        "    print(f'Exception for {pid}')\n",
        "    continue\n",
        "  audio_clips = []\n",
        "  for clip in final_loud_clips_data_test[pid]:\n",
        "    start = clip['start']\n",
        "    end = clip['end']\n",
        "    wav = processed_wavs_test[pid]\n",
        "    audio_clips.append(wav[start:end])\n",
        "    if i \u003c 3:\n",
        "      print(pid)\n",
        "      fig, axes = plt.subplots(nrows=1, ncols=2)\n",
        "      axes[0].plot(clip['window'])\n",
        "      print(start / wav.size, end / wav.size)\n",
        "      axes[1].plot(wav[start: end])\n",
        "      plt.show()\n",
        "  audio_clips_per_participant_test[pid] = audio_clips\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poUckRNgaABe"
      },
      "source": [
        "The JSON file mentioned in the cell below is created by running the following command (for service accounts)\n",
        "\n",
        "```\n",
        "gcloud auth application-default login --impersonate-service-account SERVICE_ACCT\n",
        "```\n",
        "\n",
        "or that command\n",
        "\n",
        "```\n",
        "gcloud auth application-default login\n",
        "```\n",
        "\n",
        "to identify with your own account.\n",
        "\n",
        "This assumes that you have first [installed](https://cloud.google.com/sdk/docs/install) `gcloud` CLI and created a service account (see [[1]](https://cloud.google.com/iam/docs/service-account-overview), [[2]](https://cloud.google.com/iam/docs/service-accounts-create)) (identified by `SERVICE_ACCT` above)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DR74-a8sWzqY"
      },
      "outputs": [],
      "source": [
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/path/to/your/credentials/json/file'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MpweeVCW6dK"
      },
      "outputs": [],
      "source": [
        "# Environment variable `GOOGLE_APPLICATION_CREDENTIALS` must be set for these\n",
        "# imports to work.\n",
        "import api_utils\n",
        "import eval_utils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_H7s28SXHv_"
      },
      "outputs": [],
      "source": [
        "audio_clips = np.concatenate([clips for clips in audio_clips_per_participant.values()])\n",
        "print(audio_clips.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4c_xY3D1XOOQ"
      },
      "outputs": [],
      "source": [
        "audio_clips_test = np.concatenate([clips for clips in audio_clips_per_participant_test.values()])\n",
        "print(audio_clips_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rxv1zZaqXOYm"
      },
      "outputs": [],
      "source": [
        "batches = [audio_clips[k: k+4] for k in range(0, len(audio_clips), 4)]\n",
        "final_batch = batches[-1]\n",
        "batches = np.stack(batches[:-1])\n",
        "print(batches.shape)\n",
        "print(final_batch.shape)\n",
        "\n",
        "responses = {}\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "  futures = {\n",
        "      executor.submit(api_utils.make_prediction_with_exponential_backoff, api_utils.RAW_AUDIO_ENDPOINT_PATH, batch): batch_idx\n",
        "      for batch_idx, batch in enumerate(batches)\n",
        "  }\n",
        "  for future in concurrent.futures.as_completed(futures):\n",
        "    batch_idx = futures[future]\n",
        "    try:\n",
        "      responses[batch_idx] = future.result()\n",
        "    except Exception as e:\n",
        "      print(\"An error occurred:\", e)\n",
        "\n",
        "responses[len(batches)] = api_utils.make_prediction_with_exponential_backoff(\n",
        "    endpoint_path=api_utils.RAW_AUDIO_ENDPOINT_PATH,\n",
        "    instances=final_batch,\n",
        "  )\n",
        "\n",
        "responses = [responses[k] for k in sorted(responses.keys())]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UuLNubPXwlt"
      },
      "outputs": [],
      "source": [
        "batches = [audio_clips_test[k: k+4] for k in range(0, len(audio_clips_test), 4)]\n",
        "final_batch = batches[-1]\n",
        "batches = np.stack(batches[:-1])\n",
        "print(batches.shape)\n",
        "print(final_batch.shape)\n",
        "\n",
        "responses_test = {}\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "  futures = {\n",
        "      executor.submit(api_utils.make_prediction_with_exponential_backoff, api_utils.RAW_AUDIO_ENDPOINT_PATH, batch): batch_idx\n",
        "      for batch_idx, batch in enumerate(batches)\n",
        "  }\n",
        "  for future in concurrent.futures.as_completed(futures):\n",
        "    i = futures[future]\n",
        "    try:\n",
        "      responses_test[i] = future.result()\n",
        "    except Exception as e:\n",
        "      print(\"An error occurred:\", e)\n",
        "\n",
        "responses_test[len(batches)] = api_utils.make_prediction_with_exponential_backoff(\n",
        "    endpoint_path=api_utils.RAW_AUDIO_ENDPOINT_PATH,\n",
        "    instances=final_batch,\n",
        "  )\n",
        "responses_test = [responses_test[k] for k in sorted(responses_test.keys())]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHK0pv_jX89q"
      },
      "outputs": [],
      "source": [
        "embeddings = np.concatenate(responses, axis=0)\n",
        "embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSNeAKEBX9w8"
      },
      "outputs": [],
      "source": [
        "embeddings_test = np.concatenate(responses_test, axis=0)\n",
        "embeddings_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-rMnif8X-6C"
      },
      "source": [
        "# Train linear probes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWCfrt3UYAZc"
      },
      "source": [
        "## Fetch labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m51f936RYCi4"
      },
      "outputs": [],
      "source": [
        "label_per_barcode = dfs['Metadata and Codebook/GHAI_Final_Data_2023.csv'][['barcode', 'tb_decision']].set_index('barcode').tb_decision.to_dict()\n",
        "# `barcode` column has format `XX-XXX-XX.wav`\n",
        "label_per_participant_id = {k: label_per_barcode.get(k.split('.')[0]) for k in audio_clips_per_participant.keys()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OjoCFzDYECt"
      },
      "outputs": [],
      "source": [
        "participant_ids = np.concatenate([[pid] * len(clips) for pid, clips in audio_clips_per_participant.items()])\n",
        "labels = [label_per_participant_id[pid] for pid in participant_ids]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBjakP34YE34"
      },
      "outputs": [],
      "source": [
        "len(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_q4N2wqYYNLM"
      },
      "outputs": [],
      "source": [
        "Counter(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ek51zRjtYOD7"
      },
      "outputs": [],
      "source": [
        "# `barcode` column has format `YYYYYYYYYY/XX-XXX-XX.wav`\n",
        "label_per_participant_id_test = {k: label_per_barcode.get(k.split('/')[1].split('.')[0]) for k in audio_clips_per_participant_test.keys()}\n",
        "participant_ids_test = np.concatenate([[pid] * len(clips) for pid, clips in audio_clips_per_participant_test.items()])\n",
        "labels_test = [label_per_participant_id_test[pid] for pid in participant_ids_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHLhGMsYVuu"
      },
      "outputs": [],
      "source": [
        "Counter(labels_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agAUoV5SYXDh"
      },
      "source": [
        "## Train using participant-level cross-validation\n",
        "Training data comes from `Kanyama Phone B`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXMVcq1FYaU1"
      },
      "outputs": [],
      "source": [
        "# Train on data from `Kanyama Phone B`\n",
        "w = [l is not None for l in labels]\n",
        "labels = np.array(labels)\n",
        "w = np.array(w)\n",
        "participant_ids = np.array(participant_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2csSonvzYb-j"
      },
      "outputs": [],
      "source": [
        "probe = eval_utils.train_linear_probe_with_participant_level_crossval(\n",
        "    features=embeddings[w],\n",
        "    labels=labels[w].astype(int),\n",
        "    participant_ids=participant_ids[w],\n",
        "    n_folds = 5,\n",
        "    use_sgd_classifier = True,\n",
        "    stratify_per_label = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k7K-1lManSD"
      },
      "source": [
        "## Evaluate\n",
        "Eval data comes from `Chainda South Phone B`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMvyhxJ0aLeI"
      },
      "outputs": [],
      "source": [
        "# ROCAUC per recording\n",
        "\n",
        "w_test = [l is not None for l in labels_test]\n",
        "labels_test = np.array(labels_test)\n",
        "w_test = np.array(w_test)\n",
        "\n",
        "metrics.roc_auc_score(\n",
        "    y_true=labels_test[w_test].astype(int),\n",
        "    y_score=probe.predict_proba(embeddings_test[w_test])[:, 1],\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEES26CQaQ6N"
      },
      "outputs": [],
      "source": [
        "# ROCAUC per participant\n",
        "\n",
        "score_df = pd.DataFrame({\n",
        "    'label': labels_test[w_test].astype(int),\n",
        "    'score': probe.predict_proba(embeddings_test[w_test])[:, 1],\n",
        "    'id': participant_ids_test[w_test],\n",
        "}).groupby('id').agg({'label': 'max', 'score': 'mean'})\n",
        "\n",
        "metrics.roc_auc_score(\n",
        "    y_true=score_df.label.values,\n",
        "    y_score=score_df.score.values,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1dZcVKQaejf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//medical/discovery/colab:acoustic_notebook",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
